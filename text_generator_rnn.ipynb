{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1726 characters, 45 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\x80': 0, '\\n': 1, '\\x99': 2, ' ': 3, ')': 4, '(': 5, ',': 6, '.': 7, '1': 8, '5': 9, 'D': 10, 'F': 11, 'I': 12, 'M': 13, 'L': 14, 'S': 15, '\\xe2': 22, 'T': 17, 'W': 18, 'Y': 19, 'a': 20, 'c': 21, 'b': 16, 'e': 23, 'd': 24, 'g': 25, 'f': 26, 'i': 27, 'h': 28, 'k': 29, 'm': 30, 'l': 31, 'o': 32, 'n': 33, 'q': 34, 'p': 35, 's': 36, 'r': 37, 'u': 38, 't': 39, 'w': 40, 'v': 41, 'y': 42, 'x': 43, 'z': 44}\n"
     ]
    }
   ],
   "source": [
    "print char_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\x80', 1: '\\n', 2: '\\x99', 3: ' ', 4: ')', 5: '(', 6: ',', 7: '.', 8: '1', 9: '5', 10: 'D', 11: 'F', 12: 'I', 13: 'M', 14: 'L', 15: 'S', 16: 'b', 17: 'T', 18: 'W', 19: 'Y', 20: 'a', 21: 'c', 22: '\\xe2', 23: 'e', 24: 'd', 25: 'g', 26: 'f', 27: 'i', 28: 'h', 29: 'k', 30: 'm', 31: 'l', 32: 'o', 33: 'n', 34: 'q', 35: 'p', 36: 's', 37: 'r', 38: 'u', 39: 't', 40: 'w', 41: 'v', 42: 'y', 43: 'x', 44: 'z'}\n"
     ]
    }
   ],
   "source": [
    "print ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 20 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in xrange(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(xrange(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in xrange(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " FeopIwmu1rY51M,v.q.vw5eti aDMD�m .YWSw)� vIFv r,eti\n",
      "Mr�xS Lvatlgatyc�aS5cazM,\n",
      "f�\n",
      "��,Moki.FpvYI�We S� \n",
      "----\n",
      "iter 0, loss: 76.133252\n",
      "----\n",
      " pmleamterfSees epsapleieegeheeehsshhennshlppheppteabgphgheeeeeetphrteirhirpalrpsehpepetebsembhephes  \n",
      "----\n",
      "iter 100, loss: 76.716273\n",
      "----\n",
      " inifn  nrawrt.nleTraorofniunitroel,.a  nt cr corrhanl c impaiooeastgceauhqfrolre gnceenl,ie t,georpa \n",
      "----\n",
      "iter 200, loss: 75.649193\n",
      "----\n",
      " nde caoro  eofer s et t\n",
      "se e heo)ole le taemtfaieg rop  sri m peeief ui   o m tpyoelee poeeiaoell nd \n",
      "----\n",
      "iter 300, loss: 74.303432\n",
      "----\n",
      " we tuaoshet ooer he t Lftse \n",
      "weig ry sTosceTo  rewhe thMLweur siayboSeoYei Sy murosrehe yogcolhens r \n",
      "----\n",
      "iter 400, loss: 72.928923\n",
      "----\n",
      "  �h�od eureaohe cheosot te nngatcerhash t rhf neceet thenrtetbyre v �xadhe ts e Sleed, nive ada it l \n",
      "----\n",
      "iter 500, loss: 71.436425\n",
      "----\n",
      " ngonanme onFinorna g nnlzaunhnnl vct nsnnTnnhy(n ztgsuan n z�ld bialhnurvipinngenfes tudan alithnnnc \n",
      "----\n",
      "iter 600, loss: 69.950229\n",
      "----\n",
      " eacatoTke ip, bes. fe pesemit., maaoSe u rigthi cemT me dherp iita he e T de to d pfsTe ae tu  me g  \n",
      "----\n",
      "iter 700, loss: 68.544344\n",
      "----\n",
      " fete tit ge y Ft eidhsit yh shegame f cleng fupdeTe f ntlges c qf tergt bsatart athelt d ffawf theet \n",
      "----\n",
      "iter 800, loss: 67.181531\n",
      "----\n",
      " tu md cnc\n",
      "niid pses t, otcyot tiug p a iy t the e i.py te lorepcleetl ciml fi ntloneag ace sherorot  \n",
      "----\n",
      "iter 900, loss: 65.844904\n",
      "----\n",
      "  romon, k e�Le id shThd oc Md ansep Ta bamhesraven\n",
      "d bemonSwm ar thelrpd. sulaS)l er, nn dhaMScoveen \n",
      "----\n",
      "iter 1000, loss: 64.607657\n",
      "----\n",
      " atManxionzane te pfas cy  �fsze omeyunnoearone c ak �eanaig cevon Lmy t an ieg atyonele yexeranr a f \n",
      "----\n",
      "iter 1100, loss: 63.354927\n",
      "----\n",
      " eureshodge�nthwnollenginnezcinlontentias osowiseng,rthont ehenyenencengengefoneeneane antonyhblan  g \n",
      "----\n",
      "iter 1200, loss: 62.082829\n",
      "----\n",
      " tpmellepse oce dSliesinpre thf ss thaurenr.he ielesn�ot antes temilyit agci�The ge t is elinatenu ly \n",
      "----\n",
      "iter 1300, loss: 60.873026\n",
      "----\n",
      " uttenlggasuprluukatuusatprarlurSmult.ely ind at tilts the thltilime hreaag lluet ptetheat tM�c thd a \n",
      "----\n",
      "iter 1400, loss: 59.727977\n",
      "----\n",
      " r, mu  tha.lmatile. the dhe the tis atoninet fe ant of ff, thesut  f onerippme fo\n",
      ", fos thes o\n",
      "forim \n",
      "----\n",
      "iter 1500, loss: 58.563465\n",
      "----\n",
      " y coniine mom,hmextofequer ca b momas anp, the ao tpou them, dowherhe1 or of th to orent e eforSme�c \n",
      "----\n",
      "iter 1600, loss: 57.500115\n",
      "----\n",
      " o iar av�ine dy ormeaanc Wed eue tes yef ol g�s ine, e dop, a beScantenomrel ona zome. ce�tithewal p \n",
      "----\n",
      "iter 1700, loss: 56.453418\n",
      "----\n",
      " ntigs m�wiT\n",
      "rind afontuty otitcrenagisiages, tengenrelrertime sirgetaslbearlg, i�xto\n",
      "ry gewgIalan se \n",
      "----\n",
      "iter 1800, loss: 55.428056\n",
      "----\n",
      " lo pec calpwe binu whe�eve lice limenkeciovly plplpltouidoverlont olinger andrat ane fimc. tore lime \n",
      "----\n",
      "iter 1900, loss: 54.423924\n",
      "----\n",
      "  whencint erind atd atuepsise dimuthe thsotc the they mell ghal, inesunres, theltinplldigt hatricoss \n",
      "----\n",
      "iter 2000, loss: 53.504355\n",
      "----\n",
      " e Te gor, tapluTM f atetruse nue ine..ursiIg il, wite gatpuny aipute aIb of roan met maendenfeutiet  \n",
      "----\n",
      "iter 2100, loss: 52.572701\n",
      "----\n",
      " erfoplcentae rorresspo hheroullith\n",
      "\n",
      "Ythenysmatea s bnefors at�ondiate thous on oa Somof on Thee fore \n",
      "----\n",
      "iter 2200, loss: 51.711030\n",
      "----\n",
      " a wore cudiotiof af dolen bmurs of kom 5D�a c at of foundlcorge d sorer�on e\n",
      "\n",
      "n aSemlutl otturd mepu \n",
      "----\n",
      "iter 2300, loss: 50.881003\n",
      "----\n",
      " ullu iho gad me m g the the iterb1 borllerge e f re thary de, thy yo\n",
      "\n",
      "I, be, tha moreryiunt  he the  \n",
      "----\n",
      "iter 2400, loss: 50.056073\n",
      "----\n",
      " prt ateatehindt(torocoresly socomint onrgateing ad letiny tor, to ondinplaproruttang estevy fouplerc \n",
      "----\n",
      "iter 2500, loss: 49.245886\n",
      "----\n",
      " ytese seg te Yuntlunlly poct.orentige the rr timatppunu dtoyes iberublmam eprelredsppunsiag serrinil \n",
      "----\n",
      "iter 2600, loss: 48.544595\n",
      "----\n",
      "  farcofessantsat ge f fore eu tevesiin sol uTe Ie ait forgecunge licgeneodprgate cqatp she . Tf 1s r \n",
      "----\n",
      "iter 2700, loss: 47.819265\n",
      "----\n",
      " t gof tht alyerge pod to fomiga biy po\n",
      "rsgs the sedte ixdss oforye y LSTM thaer wouy prpuse thathial \n",
      "----\n",
      "iter 2800, loss: 47.086677\n",
      "----\n",
      " lste qwet of mag.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "\n",
      "Is fo\n",
      "\n",
      "\n",
      "\n",
      "Yipete\n",
      "\n",
      "\n",
      "\n",
      "Fhcer wh�ertn s anu g�tntet, LSwop S, thysaWetoo\n",
      "put of \n",
      "----\n",
      "iter 2900, loss: 46.421345\n",
      "----\n",
      "  on iwpuriol, sevemutexfanatheg sete thexnlentSar, dSbarr may we hhe whend hall ont fimm forgen le g \n",
      "----\n",
      "iter 3000, loss: 45.764037\n",
      "----\n",
      " o get od, zentat ces. iolith limors. llime gocan, Lrentete birf cent. Iredtorges uvprever. totes bod \n",
      "----\n",
      "iter 3100, loss: 45.140607\n",
      "----\n",
      " e\n",
      "\n",
      "n)rac basreldowiigrat, oliteralresentratiplberoretedt es forpet�he sect, ceppupto ier, if gatelia \n",
      "----\n",
      "iter 3200, loss: 44.612985\n",
      "----\n",
      " ntitimerbe getl inpufqul bert the yiatsen bith, worpon (nt, tidt LSteigatkinpet teou nollith gomenpr \n",
      "----\n",
      "iter 3300, loss: 44.035847\n",
      "----\n",
      "  whe fatr  nhe s is shes.abe f s be bewh gos and d wu, o fonplustor ind rrwdinlint aseyime t, anciri \n",
      "----\n",
      "iter 3400, loss: 43.466824\n",
      "----\n",
      " t to g  iod�dkat of dorori�s bupress tite toon, of uteaves at. bagepdite orsel Lve fimfimelts songe \n",
      "----\n",
      "iter 3500, loss: 42.960096\n",
      "----\n",
      " he the gagund therentenges mamesy pettthe’ iinmers theserey god ate she ihe mel melne fore, yoc of \n",
      "----\n",
      "iter 3600, loss: 42.429795\n",
      "----\n",
      " ll. The tirgetf s as and worget m In thentet tou�mores onk sor. cousentinl f rppn le into ihe cameit \n",
      "----\n",
      "iter 3700, loss: 41.938430\n",
      "----\n",
      " tnu repnate therry rise tic ais. ss aurrconl autn ve cort ou biml. d yiryimil, ric bes cuve te thy s \n",
      "----\n",
      "iter 3800, loss: 41.506581\n",
      "----\n",
      " tigr the forlen, ielpentatet thaseithe vele thellethand calfanl ier penk\n",
      "\n",
      "Diniteiforsinclll inrend t \n",
      "----\n",
      "iter 3900, loss: 41.060563\n",
      "----\n",
      " n pumped ge horlen the cect  Thy bemofo’rt of, the buet methe en thencer w ddetsion on aed malluml \n",
      "----\n",
      "iter 4000, loss: 40.590390\n",
      "----\n",
      " relof nt w ve aipraivare of sagaIns neve pats aSceves. Ta meplk ple ide fipl so ge hats ol Tinc ore  \n",
      "----\n",
      "iter 4100, loss: 40.198559\n",
      "----\n",
      " bing ontet the es mepeethe cent w�efimes ethForekment of wotf to zone cawale the coythathe gh c morl \n",
      "----\n",
      "iter 4200, loss: 39.762427\n",
      "----\n",
      " t’nger’ratel ficerewDinl forent. Titiy foraeregmenclesenkerryorne dicadructorentare, itpueforess \n",
      "----\n",
      "iter 4300, loss: 39.364259\n",
      "----\n",
      " sha tad LSTM te linline irry relgy mopF\n",
      "me sratseateply as of is molghte ineto. n�end gat’rd the f \n",
      "----\n",
      "iter 4400, loss: 39.026051\n",
      "----\n",
      "  Iulgaina.\n",
      "uTh lu pfop ate. rile. morcumpue his cueriule its isetiti hirgetetide, opliaclonioruis an \n",
      "----\n",
      "iter 4500, loss: 38.671206\n",
      "----\n",
      " Dames pler.\n",
      "s, a cenre� polihats of ate on.\n",
      "\n",
      "lis sor\n",
      "\n",
      "\n",
      "ulior kopmaseingatrestes why te of reeve font \n",
      "----\n",
      "iter 4600, loss: 38.295221\n",
      "----\n",
      " o ses on ony 5 shs pSwasler link os cornen she she itereqiod bith ig ant LzeWene enmes siplet whe t, \n",
      "----\n",
      "iter 4700, loss: 37.951462\n",
      "----\n",
      " \n",
      "n, docemenger on the the d onedisf cendwhmon thexr)Fls, be eent the sod cuthztad whadsen se totee n \n",
      "----\n",
      "iter 4800, loss: 37.639181\n",
      "----\n",
      " tharmer, the linghimere tire cire, cen workeser ther, comene hesthe hay fhe tnd mus onele. If ass, t \n",
      "----\n",
      "iter 4900, loss: 37.310939\n",
      "----\n",
      " emprertitipate stortining one somimermlles ay nill fiy whe tapus d e purrestac�asent mells. (The gec \n",
      "----\n",
      "iter 5000, loss: 37.016818\n",
      "----\n",
      " at dants e celrerte cegad tatevedponaet en of ye ntett the giply one tod igesles anpat, armingiorset \n",
      "----\n",
      "iter 5100, loss: 36.731982\n",
      "----\n",
      " pu thay celiserne iplcet maIf the itatemerganus Lhe bidset kilitacestecat te cemes ine ti.ttren, aut \n",
      "----\n",
      "iter 5200, loss: 36.409096\n",
      "----\n",
      " thumus the fopll of y ove dend, forgat. LSTMs oS mall of LStorugk te socangony stat wiios oherrent b \n",
      "----\n",
      "iter 5300, loss: 36.141015\n",
      "----\n",
      "  ely)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dingen yotser god mace cense the forpac nethe te iodwhate’s sto sexren tf to the as mo \n",
      "----\n",
      "iter 5400, loss: 35.909138\n",
      "----\n",
      " trantt cines sipllravere, the asenwel sacigerrind, ted on Wele thexrloput whent atesers sher.)\n",
      "\n",
      "Ds a \n",
      "----\n",
      "iter 5500, loss: 35.657809\n",
      "----\n",
      " on onlly mrant dime qurgatengin� tomu ditl dict aztinlerenting orres on cul ore sialestosenter gover \n",
      "----\n",
      "iter 5600, loss: 35.380019\n",
      "----\n",
      " ul timeroget ans pde cugmelts timtiprmptecray socutst thbs LSTMs tipliat ooit det LSTMs cus rumand.  \n",
      "----\n",
      "iter 5700, loss: 35.179573\n",
      "----\n",
      " rsexe’s tipuriave fime ttorerag qemef a cuure sec itimesdequinbisiforcone sha d ofuren it shate es \n",
      "----\n",
      "iter 5800, loss: 34.928427\n",
      "----\n",
      " raddifa, of dad LSTMs, qnith is aagigats orto s borputforgesthay the former on in and ofouthats thel \n",
      "----\n",
      "iter 5900, loss: 34.713617\n",
      "----\n",
      "  sepre stats avencant to forment, thats gs ok ontene se ehmurpk next hoaser You �f nelimave ment ixt \n",
      "----\n",
      "iter 6000, loss: 34.490874\n",
      "----\n",
      " s inl oat to nelint oout tor WeTe ont, whe pedsnore sisde set pingco\n",
      "Wes arrart. nn hatesore, es the \n",
      "----\n",
      "iter 6100, loss: 34.320144\n",
      "----\n",
      " coplic Ifiy 5ren’ mebsenentes relatime nor is daee socllsxassiblon rotientirkotianto shaves text c \n",
      "----\n",
      "iter 6200, loss: 34.067239\n",
      "----\n",
      "  LSTM pemoFuses becof or, it. cnangicadtak, cey on’s ruse wheserint stas mungernkinith The binntou \n",
      "----\n",
      "iter 6300, loss: 33.936570\n",
      "----\n",
      " s asemem itate ihatibe fory ine tive hirput hacampge sinp ilthe es yot hacqu tou of melis thally the \n",
      "----\n",
      "iter 6400, loss: 33.714888\n",
      "----\n",
      " elry sey methe rag ford the fof imeliLSure sorrentitr Mzwn as alem mome inge be terom.\n",
      "\n",
      "\n",
      "Dzioserrant \n",
      "----\n",
      "iter 6500, loss: 33.511371\n",
      "----\n",
      " a ofe mom mopof ot the isag sec ort weske ceasenoputse se teting ond, tof wo orue. Th to Surso lof m \n",
      "----\n",
      "iter 6600, loss: 33.311238\n",
      "----\n",
      " rgo the dicver wamas belthe eve., ame ois ats himper e catss  henrene iis, the a,dpen, art the fore  \n",
      "----\n",
      "iter 6700, loss: 33.117212\n",
      "----\n",
      " t te t. thes arrzingiteterotimathe rel onleninstore thadrod 1 tate wherrfardont to tocenctisseriyima \n",
      "----\n",
      "iter 6800, loss: 32.905295\n",
      "----\n",
      " g send, tha cemples itc os gatenrer wore radt.\n",
      "\n",
      "In sicpuskatt putiou bor bengetian inpants the forgo \n",
      "----\n",
      "iter 6900, loss: 32.796827\n",
      "----\n",
      "  tiat ascoukestases ats by pustent sinerditiwhann (aure cute T. (Fucpon wosking if itrinipry reseput \n",
      "----\n",
      "iter 7000, loss: 32.653942\n",
      "----\n",
      " ts odeculpy the gadeigo wy the goee azyim bd of sirriliof corcuplinforipuy if secupwhidtem hall io h \n",
      "----\n",
      "iter 7100, loss: 32.497770\n",
      "----\n",
      " StMs.s .et mulforgete thet the she conpedtenverwos cus ou corsent os on cucre.rumpet hand ont rred a \n",
      "----\n",
      "iter 7200, loss: 32.291480\n",
      "----\n",
      "  drel ove peve evever.\n",
      "\n",
      "Yzimplgete The bomularrent 1 the dom mxatt hitM thasts sStMsMnligeniated tat \n",
      "----\n",
      "iter 7300, loss: 32.149741\n",
      "----\n",
      " s ate ncenrekinglorpor yorer Wedseithist gats ang forgatenn’rkat whe cors, deti1 tofiny del zicay  \n",
      "----\n",
      "iter 7400, loss: 31.974106\n",
      "----\n",
      " esey multinnel lite momel iop bedsenthe therrend of Lhe m gorrenrll, be the ropus momaatrants ci.den \n",
      "----\n",
      "iter 7500, loss: 31.882463\n",
      "----\n",
      " rs nus, bt thintinise nzint of oforeinpur., ouppus, therting cirfonke wsenkey yorronthist ghanywore  \n",
      "----\n",
      "iter 7600, loss: 31.691008\n",
      "----\n",
      " m cey oSTinl oormoneu toren of lite celpuere� gewt relpor de seg come at serlen mulffrmor, ie forbib \n",
      "----\n",
      "iter 7700, loss: 31.524241\n",
      "----\n",
      " teWeshe copreith moly ontt whey puthewt hory magd bs omet to liovers cus shes, and of the atd mugt a \n",
      "----\n",
      "iter 7800, loss: 31.416062\n",
      "----\n",
      " he sed gote to le the ceckereer ongen sor inte is the otwonge thend melpiov at, ore rests ove figo h \n",
      "----\n",
      "iter 7900, loss: 31.301026\n",
      "----\n",
      " pte.\n",
      "\n",
      "Fuigat angates tnter ciat tore menning her loc o , the cog, Lhe forverrents angent’re comy t \n",
      "----\n",
      "iter 8000, loss: 31.147048\n",
      "----\n",
      " pline of magecrors on camerr to thencinf renred thel of roplimpraiaplind on ones mant of mel nner in \n",
      "----\n",
      "iter 8100, loss: 31.108841\n",
      "----\n",
      " nifn. Iowy a centionsented of magt mume gete. tipumple stere seplestalld qumag ply whagr it of oninf \n",
      "----\n",
      "iter 8200, loss: 30.987816\n",
      "----\n",
      " f 1 ahe ham et aussereing of may reltr ds oraardite ded the ty on town, LSTe whent theciof campreloe \n",
      "----\n",
      "iter 8300, loss: 30.808121\n",
      "----\n",
      " onsto batt apank That gate, and theve the mury the semacotrdith soercorpationse themprelky onkurte.) \n",
      "----\n",
      "iter 8400, loss: 30.666573\n",
      "----\n",
      " en whe fordet whe sidd ot the romumpenemamel the tuto ave fure, bopemend, te the ge to eray oSus hap \n",
      "----\n",
      "iter 8500, loss: 30.554918\n",
      "----\n",
      " t’s zel forget pury to S cedfermes any y arthenk, audto gire by LSTMs bren zire, oferry and. n yad \n",
      "----\n",
      "iter 8600, loss: 30.459380\n",
      "----\n",
      " ces socets is of in lesemilgc Iner whe inatenat may s ang cirmor gate magans aten pats�ise hhe topum \n",
      "----\n",
      "iter 8700, loss: 30.392362\n",
      "----\n",
      "  Ire the cucwes filg the sec g te bat. If arloues roputee the fillinge is crisensicler gipl pumpropr \n",
      "----\n",
      "iter 8800, loss: 30.265254\n",
      "----\n",
      " l aitsel hor. relles of neltisantiam in of repuesente be the horaire sirgmore tha, you e. (Te cuse p \n",
      "----\n",
      "iter 8900, loss: 30.127295\n",
      "----\n",
      " aicerens or anprantite ant cels ate m gaat be ot haef reple wangat thes, wherry therreats.ed thoning \n",
      "----\n",
      "iter 9000, loss: 29.997841\n",
      "----\n",
      "  eresiant��rmpltste niske, that LSThple onk foreat the d af whetstat’s the forghardint n the thand \n",
      "----\n",
      "iter 9100, loss: 29.890315\n",
      "----\n",
      " y meleerell shay come sentire cus nith moples, thar soc moll. The pente uidsen torops cey whereerest \n",
      "----\n",
      "iter 9200, loss: 29.839799\n",
      "----\n",
      " emorretlmapre, of yiplion, tow the forget mulientiteser gopimend cec it iof 1 andiagd, cey sncus rom \n",
      "----\n",
      "iter 9300, loss: 29.761571\n",
      "----\n",
      " anl ot ina the fiat worme the tamermet wicat. The cus gate gate murperets ou�dzteprallps bira inam.  \n",
      "----\n",
      "iter 9400, loss: 29.672825\n",
      "----\n",
      " eves, cey aid, of cTom \n",
      "\n",
      "Yforgathy ripnes it sec oren, tore tipurpate ff.)urrent onensefur foreprelt \n",
      "----\n",
      "iter 9500, loss: 29.513501\n",
      "----\n",
      "  gire avat mplnerestor, remownsor the them liore, azpro sea d)\n",
      "\n",
      "You of sign aty as anlicereprSputra� \n",
      "----\n",
      "iter 9600, loss: 29.373480\n",
      "----\n",
      "  as a dent aac mamll seatenesor pr baty ane tite inf resse nad auth mave . mt dedidd axtite wexf m c \n",
      "----\n",
      "iter 9700, loss: 29.306204\n",
      "----\n",
      "  ndinl manteclin. Iheprel woresientoqi5m iac go gateskent gete’ pinge shel hackiensefinghe dets on \n",
      "----\n",
      "iter 9800, loss: 29.167421\n",
      "----\n",
      " aStilgoferror Snt hall oreat ohing azgo whe gate willer carditiserrets at mupy mus sirp orcutreltant \n",
      "----\n",
      "iter 9900, loss: 29.027433\n",
      "----\n",
      " uln the st vet ent temply mallimeng tests puby mumelre, the ing by thk stpuefe thgerrplllet topust p \n",
      "----\n",
      "iter 10000, loss: 28.981181\n",
      "----\n",
      " r es meprelt muru to ift of put, a, teren ine (STMeroinathinntantsourmortitf ry th nut nd cul best w \n",
      "----\n",
      "iter 10100, loss: 28.810313\n",
      "----\n",
      " ally attou h wo aen of LSTM pigfore, sete ane topumpor, mudtha ficpuraprme oo the forgpke bmpon, of  \n",
      "----\n",
      "iter 10200, loss: 28.662765\n",
      "----\n",
      " nver gac inc os of aghed ncidl a diant cemput horet onill a hod on awer it and ouraw Thas oo dird th \n",
      "----\n",
      "iter 10300, loss: 28.561817\n",
      "----\n",
      "  gay thell get. shenwithenty whe, enter gate bems ot hopen, teput, the rat dirt toply ast be trple,  \n",
      "----\n",
      "iter 10400, loss: 28.425345\n",
      "----\n",
      " (mlg th ce wove ancent of the forrer gintinkshe bedt ts yas, tim, gate wird is cuthe lents titd afal \n",
      "----\n",
      "iter 10500, loss: 28.360720\n",
      "----\n",
      "  bope yate one tevessepte the tht reltere tecuppmulo. If.e simply a ciml senghy rulereptiments aurdi \n",
      "----\n",
      "iter 10600, loss: 28.362856\n",
      "----\n",
      " e ontzans of the ghatiager gate ig aedicutawnwore tou. The gopailea tiga.)\n",
      "\n",
      "Diofard n tist of y mant \n",
      "----\n",
      "iter 10700, loss: 28.244593\n",
      "----\n",
      "  len ther ghats ore l alcors ate be senssipltiave decon antsimy reproth llitite cerfores a stay selp \n",
      "----\n",
      "iter 10800, loss: 28.136493\n",
      "----\n",
      " tatina dicll pumenesor, of 5e hac toment the cedsent the is ak to zel cimput. If ment te cimplezt te \n",
      "----\n",
      "iter 10900, loss: 28.030875\n",
      "----\n",
      " me fomalioce. If thats os on ant the didy soputthalmave centinp have forbast henpun, ate., alka, mas \n",
      "----\n",
      "iter 11000, loss: 27.964059\n",
      "----\n",
      " te is a corstate. bs, illeriten len tow\n",
      "\n",
      "Yfurdherses nol ioftigitmed to forghtathiat ind ther ind ma \n",
      "----\n",
      "iter 11100, loss: 27.846835\n",
      "----\n",
      " d cimplet sempy you ioneent cens overrent dirgiclint calge pon, akyzurmellimple, andimplerant akmat. \n",
      "----\n",
      "iter 11200, loss: 27.818589\n",
      "----\n",
      "  oocut..\n",
      "\n",
      "In thiclcaishortores hant, auds.rutahiltimplien, the ite gate whe forgefidion, thes ighang \n",
      "----\n",
      "iter 11300, loss: 27.717858\n",
      "----\n",
      " td biccide ws. n the, wall therereackile by Lhaurset thy andiffe.)\n",
      "\n",
      "Diffeippropailw prby ohetiof 1 a \n",
      "----\n",
      "iter 11400, loss: 27.586180\n",
      "----\n",
      " gd must .s oformesterd the, and by muqu of ince the forgite gowM foushes, forget, forgeve mant gatis \n",
      "----\n",
      "iter 11500, loss: 27.523830\n",
      "----\n",
      "  tha, bhu is Ihe forofop twer prest bemoplerwmte hot, a’nd tone, the mennes coon, audforgotpurenn  \n",
      "----\n",
      "iter 11600, loss: 27.438849\n",
      "----\n",
      " ionl ttands and oned the forgpre ancat to bedrant dits aldiprel moplllmand of that teles ait if werr \n",
      "----\n",
      "iter 11700, loss: 27.313407\n",
      "----\n",
      " ’rticplprgprepreltp\n",
      "\n",
      "In stenpunky prmend aathheripp(The dofzingatime dianend moplesand or angire b \n",
      "----\n",
      "iter 11800, loss: 27.302611\n",
      "----\n",
      " The bubser presereproprepreve eirfiin ink imasethingenileseng’�gere cuserilly LSTMs tine hac iipl \n",
      "----\n",
      "iter 11900, loss: 27.239629\n",
      "----\n",
      " u momulthe cemper ther seat if to e formput. Inition core ofimput curt couswos ot biprelreip, tige h \n",
      "----\n",
      "iter 12000, loss: 27.156604\n",
      "----\n",
      " t, od on zurgatete.,ioforpof sowis ther, must pumpantise hatse to to , forget mon, tok on (Sutse the \n",
      "----\n",
      "iter 12100, loss: 27.121954\n",
      "----\n",
      " hemene come ing mentits pute, pute, pus arserevenverrmatsdinay thermplforyind ceth thend thots at’ \n",
      "----\n",
      "iter 12200, loss: 27.096224\n",
      "----\n",
      " lto, ling. The dse oh lo cocenget, onge by ff gate the to nully the forenthes hidsene’rdsor them n \n",
      "----\n",
      "iter 12300, loss: 27.017818\n",
      "----\n",
      " forges, snes of ancutimpll on (Ste blann bucanclore beltases harasteng onfas n bt ave relocing orioc \n",
      "----\n",
      "iter 12400, loss: 26.948058\n",
      "----\n",
      " forget gate whe d. forgatesimples bimonmentiol fiplitien ate the the, pumen ine cus fiwt’s the the \n",
      "----\n",
      "iter 12500, loss: 26.948952\n",
      "----\n",
      " r pline topy sianen inl custip. llen, rellinelmuve fortan ef ancon ther pel and forpen, ancler ghe c \n",
      "----\n",
      "iter 12600, loss: 26.876641\n",
      "----\n",
      " en at wove ces, a�mcgn ghes antonis ing sugane tharuste forgenc.att 5 be a bias 5 mal inay bepne ha \n",
      "----\n",
      "iter 12700, loss: 26.817921\n",
      "----\n",
      " epllee to ghe eo ao weve bus tzer the wofore, times and the diant mest purgec, sepre the ceate tis t \n",
      "----\n",
      "iter 12800, loss: 26.720636\n",
      "----\n",
      " t’s ertor te ligithe plmencwortis bs’ry rrent the nhenr muds ohe)\n",
      "\n",
      "Yo veith ms b pument c bs of  \n",
      "----\n",
      "iter 12900, loss: 26.633726\n",
      "----\n",
      " ghouc forgat, the ts on input, allor the hamant aren by mave compinethe st il of andind curret’rmp \n",
      "----\n",
      "iter 13000, loss: 26.592328\n",
      "----\n",
      "  Weyt. leentithe thesiagforsiffereprel a esept. (mat ition arast sec ciot hend. lueppdion baterostas \n",
      "----\n",
      "iter 13100, loss: 26.534541\n",
      "----\n",
      " us forget is nititiplay Lhaille put, as swe bo ofy nf oo proforeformofo, of bistred.)\n",
      "\n",
      "Diof 5ion bga \n",
      "----\n",
      "iter 13200, loss: 26.429026\n",
      "----\n",
      " bel roplmagforef any ripuiple harl of 1 thave ces anl herdstind foreite hoplu whe\n",
      "\n",
      "Yot horsto geves  \n",
      "----\n",
      "iter 13300, loss: 26.312529\n",
      "----\n",
      "  cet ant hore eve forgatt g besoren ink nd, fadseathe..\n",
      "\n",
      "In then S\n",
      "\n",
      "Yow, otiy sec zorce mutihe hand, \n",
      "----\n",
      "iter 13400, loss: 26.240960\n",
      "----\n",
      " te wh she cen, formast mante ind the tediba, sec, thasd igd inent correnspcestsing fing of silpof di \n",
      "----\n",
      "iter 13500, loss: 26.183174\n",
      "----\n",
      "  gate staser and dhe forger e by sec osikf inge ghas LSTMs hand of muse. The sind ns ones ford seice \n",
      "----\n",
      "iter 13600, loss: 26.091108\n",
      "----\n",
      " at whene dite, socent steserewny a de ted onticlmapreiancempan the bipply tnac be tecly yomertiant w \n",
      "----\n",
      "iter 13700, loss: 26.049409\n",
      "----\n",
      " be ouer whign aurplifts ony nelus at worgherreserel momuraere erticwi.d if wepant afere thases on th \n",
      "----\n",
      "iter 13800, loss: 26.021912\n",
      "----\n",
      "  nist (Thats ff 5�n ghe maTMtariwtskathe becueverros hatl hagionces ate mention eversor cal, soople \n",
      "----\n",
      "iter 13900, loss: 25.965280\n",
      "----\n",
      "  halliccus gath the .\n",
      "\n",
      "IIt deffuiwe centint com Sts busoren whert. The cuat ofur latestt w ckangs on \n",
      "----\n",
      "iter 14000, loss: 25.978317\n",
      "----\n",
      "  The the sy the gave formor woro set hopreprelen gforey the vh\n",
      "\n",
      "Yinke whod aSk worgite havese shers  \n",
      "----\n",
      "iter 14100, loss: 25.877880\n",
      "----\n",
      " ot ait hiciy tises be cemppur, to erby somumple, you to diand the goll oro, and to iz.\n",
      "\n",
      "Ytienseis. T \n",
      "----\n",
      "iter 14200, loss: 25.823228\n",
      "----\n",
      " urt agd the ling’reiant thelitete ginn diting is niurtes cellanpres mad the the diant tatpouigite  \n",
      "----\n",
      "iter 14300, loss: 25.819780\n",
      "----\n",
      " ns onsentelereplat pu fepasey ccestoules. nt diffur ther, tay ony forelrallrimange the titl iet aten \n",
      "----\n",
      "iter 14400, loss: 25.711615\n",
      "----\n",
      " ag singen sopuast me fimplusor m murshalen cusron, of come towMrauserestopro tw forefor fam ow, indi \n",
      "----\n",
      "iter 14500, loss: 25.650056\n",
      "----\n",
      " iment. le tag LSTMs orwery mave pugpcon Stequ ofille be oot wos then., for cay Luen, of wi�rpuuthe n \n",
      "----\n",
      "iter 14600, loss: 25.601119\n",
      "----\n",
      " ng the to zero shate, sec mxt hand of the ceoret. theeluene, one profores cend, sequen, dothe forkhe \n",
      "----\n",
      "iter 14700, loss: 25.530047\n",
      "----\n",
      " rdprencenetior thating d ang to that sec inate entMsingen anl o wes nu cere ancu sec hadd, she gaken \n",
      "----\n",
      "iter 14800, loss: 25.476602\n",
      "----\n",
      "  the is putsel d forget gets fimplgeten the.snttoudyice c in inl oiye mant ants il. Titn maltingite  \n",
      "----\n",
      "iter 14900, loss: 25.458781\n",
      "----\n",
      " e of aedMicd gota gay cay theullinilis stes. The solutandsoullorem nerrestoren aruqu dite makt blepl \n",
      "----\n",
      "iter 15000, loss: 25.376731\n",
      "----\n",
      " ust whathe she gopimpropcice forget mam purpigiopreve diff.\n",
      "\n",
      "dias cire. Sfer waytife, and one forve  \n",
      "----\n",
      "iter 15100, loss: 25.298244\n",
      "----\n",
      " aunsat. Sut, t’rcented aler of .\n",
      "\n",
      "\n",
      "\n",
      "Ynd to out besocine cere inttiof ropcucpuvenfirponed thes and  \n",
      "----\n",
      "iter 15200, loss: 25.199068\n",
      "----\n",
      " amant tetr wented af the theny a detsentithy she pe thats ouation, as is on the rebs oh (Santhes hid \n",
      "----\n",
      "iter 15300, loss: 25.112486\n",
      "----\n",
      " nt soce, hant of lime docugpat and mulc imant, doforest what wo gnd toth rowislerensen sias is a(bel \n",
      "----\n",
      "iter 15400, loss: 25.030486\n",
      "----\n",
      " ge is belmepthiyingink nisoaightepuathe relliopks dhe teby angs inple berowsers ats cimpmelrrmented  \n",
      "----\n",
      "iter 15500, loss: 25.031328\n",
      "----\n",
      " s beats cust rel rwone iplpigpreser intime es any is putleneenk thegf LSTMs. The gatl of s fprmite f \n",
      "----\n",
      "iter 15600, loss: 25.001923\n",
      "----\n",
      " ir thputing for prol oo (The havel imples pweithcur bepre foren muga hure’s towming is slene gise  \n",
      "----\n",
      "iter 15700, loss: 24.947848\n",
      "----\n",
      " ne mand. us if ay Lhay rel ding. The cumpune theforoa bo zen whathhere tow, foref you compen ane exa \n",
      "----\n",
      "iter 15800, loss: 24.915275\n",
      "----\n",
      "  ond on’rmeftirt oSlud thy relle the to zent, fongences ame e. laten, cumplu be bed aof ant to h m \n",
      "----\n",
      "iter 15900, loss: 24.821885\n",
      "----\n",
      " hersite. s.)\n",
      "\n",
      "Dha the s, of , on tore telon siry therrefer lad LSTMs�n, ces adt Thatithiy sef ropan \n",
      "----\n",
      "iter 16000, loss: 24.776694\n",
      "----\n",
      " icpcorn roy the forge sey daserers theclinge ther ite is ate formast sout, une campad of lithy ropur \n",
      "----\n",
      "iter 16100, loss: 24.753791\n",
      "----\n",
      " The forgfore, wifo, anding cirllenthat qu of deforkfopreves ang them, ontequen, of dat soc inyou wo. \n",
      "----\n",
      "iter 16200, loss: 24.673986\n",
      "----\n",
      " d anttipl aweser hamertine hiby putirasestom (The es of iore for oo sheserens celly LSTMs hay Ln imp \n",
      "----\n",
      "iter 16300, loss: 24.594550\n",
      "----\n",
      " t fam celt whe tebdene, pry at. In bs anto ind or dot. tiof and andites roplimes at of intorets ot d \n",
      "----\n",
      "iter 16400, loss: 24.522219\n",
      "----\n",
      " hits neseats f vec mam ne to nhe asdithes hasey bs for gatesees, and ats of yop�ofes cels. to the fo \n",
      "----\n",
      "iter 16500, loss: 24.453857\n",
      "----\n",
      " tt ind ropursente litstose to buste ghtere, at tforet’�dithe inge incen, alyirr, te the d.sastes i \n",
      "----\n",
      "iter 16600, loss: 24.388994\n",
      "----\n",
      " propemply ovets birn sdtpresents repre rats and ortac e thout the hind if adyou to n by sopresan, al \n",
      "----\n",
      "iter 16700, loss: 24.383755\n",
      "----\n",
      " ineve dercerroncen wicand tipliegrol’s gate forget gate to zed the romzes neramen besy rel es n th \n",
      "----\n",
      "iter 16800, loss: 24.373859\n",
      "----\n",
      " hersioplyofe gaty tion the sfas of line dialiapucincropcus nisormon ind (Scucwes of you iolimencem g \n",
      "----\n",
      "iter 16900, loss: 24.270887\n",
      "----\n",
      " us ofingrtithertes cemero nhpress ink foreserte putimelte shc.. Inpungh to zorelro topumengetiplif t \n",
      "----\n",
      "iter 17000, loss: 24.183233\n",
      "----\n",
      " e tinced es ave forget ticutse thby weredidle the forvertite ofoy thenk mopketh theverrove geth do i \n",
      "----\n",
      "iter 17100, loss: 24.095091\n",
      "----\n",
      " t’s ion the puethend oScusseate hh�d er of aSdenal the ds liplunimole. Its of mave forge wiafes a  \n",
      "----\n",
      "iter 17200, loss: 24.016306\n",
      "----\n",
      " cimply ancemplas.a, incuu pugbeld whal thy cas ingat herant aipk mung, id ineng iforeforens inans a  \n",
      "----\n",
      "iter 17300, loss: 23.964470\n",
      "----\n",
      "  (Stiserou’s cedsentite ot azinger axalks, to diant cimpuples, pleses es of imtt it allias of inte \n",
      "----\n",
      "iter 17400, loss: 23.938576\n",
      "----\n",
      " utpertprel pupthiepuant of and fopuitillion, of relamwony lesero lidithmuse, pume thellpwordite ropu \n",
      "----\n",
      "iter 17500, loss: 23.910368\n",
      "----\n",
      " proforpenth theveimest putiag hore.)\n",
      "\n",
      "You pugpeneete gatl moltershastorent socums, pampresenita, shy \n",
      "----\n",
      "iter 17600, loss: 23.861527\n",
      "----\n",
      "  ts ohe, ser cent of masditd relinton the w, bu to zed dMsor bran be the forper hay a shertMsot corm \n",
      "----\n",
      "iter 17700, loss: 23.794272\n",
      "----\n",
      " c decles, ne foreplis them, talun thand mople as gig, pre cey sicbe seat sec mopre. St han beim nec  \n",
      "----\n",
      "iter 17800, loss: 23.737597\n",
      "----\n",
      "  les gppus. rden yor’res rupresent to impreve .\n",
      "\n",
      "SThe diag pigl as allon, yo . bsta tray LSyireser \n",
      "----\n",
      "iter 17900, loss: 23.671341\n",
      "----\n",
      " at atiset thecdilges, ily ropaprepres biast pumprot wtite ciseget pemertith. Ineted to yor bepre fop \n",
      "----\n",
      "iter 18000, loss: 23.632502\n",
      "----\n",
      " that putimay LSTMs, whel seclmagfer foutore , at ieluy munty firalerdac the niplus aftigg andsormemo \n",
      "----\n",
      "iter 18100, loss: 23.530088\n",
      "----\n",
      "  forke whe aipres bs bell bean. the diantime fore ther if iets fordita, arwzorserwomprelrmand, for w \n",
      "----\n",
      "iter 18200, loss: 23.459541\n",
      "----\n",
      "  dith dherented ats oferstewn dodsen ata bs on tote ped ant celth thendsdathe sses of cay of 1 purei \n",
      "----\n",
      "iter 18300, loss: 23.443994\n",
      "----\n",
      " ske ana gagd whe the d and come therenthy LSy repleses cemes atanf repreformercestigl may the tigge  \n",
      "----\n",
      "iter 18400, loss: 23.368435\n",
      "----\n",
      " of LSTMs o tha socinge the dialt gand the stats, inother. the fore tecorent domant te ind cay the se \n",
      "----\n",
      "iter 18500, loss: 23.274889\n",
      "----\n",
      " ullims timget inat sent des cur mestric incem, a dicpleve enave pus tit rustipls and complet enas on \n",
      "----\n",
      "iter 18600, loss: 23.302880\n",
      "----\n",
      " niples anes wok pulied, uerent celles nel iet, illis ces rele whel. Stequent ceme, ping itputprepref \n",
      "----\n",
      "iter 18700, loss: 23.214127\n",
      "----\n",
      " attinliof woro the diga reformesocustmmos of a d ane diag sor thplical no ever of if ropem on the st \n",
      "----\n",
      "iter 18800, loss: 23.166145\n",
      "----\n",
      "  forget gete inaTd anting oren ouen to forkif , antior ofot forefore, w rcon tote tom matfore, whe g \n",
      "----\n",
      "iter 18900, loss: 23.150756\n",
      "----\n",
      " nd by n seck, the putsiatse pro the diant cemply as aiprel a forget gete if wes hicu ganc es bs one, \n",
      "----\n",
      "iter 19000, loss: 23.123130\n",
      "----\n",
      " borerresimple. If 5. Wocimpre. St sor yay oveor bs on aWdan, secees a biaclus thoncdalts and cell ro \n",
      "----\n",
      "iter 19100, loss: 23.108587\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-63e6f7340574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'iter %d, loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-63a5ae523ec5>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m \u001b[0;31m# unnormalized log probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# softmax (cross-entropy loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;31m# backward pass: compute gradients going backwards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 100)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print '----\\n %s \\n----' % (txt, )\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
